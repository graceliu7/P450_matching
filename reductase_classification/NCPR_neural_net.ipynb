{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4dffced3618d713eb35573249f37da055ccb4ce8327966404b9b643de5d1677c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NCPR_functions import load_NCPR, train_test_split\n",
    "\n",
    "from river import metrics, preprocessing, stream, linear_model, tree, ensemble, compat, compose\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import model_to_river\n",
    "import my_pipeline\n",
    "from ensemble_class import EnsembleModel\n",
    "from torch import nn, optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data, NCPR_df = load_NCPR('data/NCPR_bert.npz', 'data/uniprot-NCPR.tab', 'data/uniprot-NCPR.fasta')\n",
    "xtrain, ytrain, xtest, ytest = train_test_split(dict_data, NCPR_df)"
   ]
  },
  {
   "source": [
    "def river_nn(X,y, net):\n",
    "    model = compose.Pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        compat.PyTorch2RiverRegressor(\n",
    "            net=net,\n",
    "            loss_fn=nn.MSELoss(),\n",
    "            optimizer=optim.SGD(net.parameters(), lr=1e-3),\n",
    "            batch_size=2\n",
    "            )\n",
    "        )\n",
    "    model = model.to(device=torch.device('cuda:0'))\n",
    "    metric = metrics.Accuracy()\n",
    "    for i in range(len(X)-1):\n",
    "            train_stream = stream.iter_array(\n",
    "                X[i], y[i],\n",
    "                feature_names = ['x{}'.format(j) for j in range(len(X[i]))] \n",
    "            )\n",
    "            \n",
    "            for data, target in train_stream:\n",
    "                x = model.transform_one(data)\n",
    "                model = model.learn_one(x, target)\n",
    "\n",
    "    test_stream = stream.iter_array(\n",
    "        X[-1], y[-1],\n",
    "        feature_names = ['x{}'.format(j) for j in range(len(X[-1]))] \n",
    "        )\n",
    "    for data, target in test_stream:\n",
    "        y_pred = model.predict_proba_one(data)      # make a prediction\n",
    "        metric = metric.update(target, y_pred)\n",
    "    return metric.get()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.y)):\n",
    "            yield self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyIterableDataset(xtrain, ytrain)\n",
    "test_data = MyIterableDataset(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(768, 900),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(900, 1100),\n",
    "    nn.Linear(1100, 1274),\n",
    ")\n",
    "X = [xtrain, xtest]\n",
    "y = [ytrain, ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "cuda0 = torch.device('cuda:0')\n",
    "net = net.to(device=cuda0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(cuda0)\n",
    "        target = target.to(cuda0)\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'river_nn' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-89dc4a9b525a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mriver_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'river_nn' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# Turning off automatic differentiation\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(cuda0)\n",
    "        target = target.to(cuda0)\n",
    "\n",
    "        output = net(data)\n",
    "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "      (test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}